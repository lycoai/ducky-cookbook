

ğŸ—“ï¸ Product Strategy Workshop â€“ Thursday, 6 June 2025, 9:30 AM (PKT)

Participants
â€¢ Imran (Head of Product)
â€¢ Sarah (Product Manager)
â€¢ Faisal (Sales Ops Lead)
â€¢ Maria (Growth Marketing)
â€¢ Ali (Backend Engineer)
â€¢ Tania (Frontend Engineer)
â€¢ Usman (ML Engineer)
â€¢ Hassan (QA Engineer)

â¸»

Imran (HoP) â€“ Opening & First-Principles Framing

â€œLetâ€™s strip this down. Why does lead scoring exist? Three atomic goals:
1ï¸âƒ£ Relevance â€“ reps must know whom to call first.
2ï¸âƒ£ Efficiency â€“ cut manual triage to near-zero.
3ï¸âƒ£ Learning loop â€“ every closed deal should improve the model.
If any feature doesnâ€™t serve one of those, itâ€™s noise.â€

â¸»

Sarah (PM) â€“ Vision & North-Star Metric

â€œOur north-star: Lift SQL-to-Win rate by 20 % within two quarters.
	â€¢	Phase 1 (Q3): Deploy ML-based scoring badges inside Lead Detail.
	â€¢	Phase 2 (Q4): Auto-prioritise the rep work-queue.
	â€¢	Phase 3 (Q1 2026): Predict next best action (demo, email, call) per lead.â€

â¸»

Faisal (Sales Ops) â€“ Pain-Point Validation

â€œReps burn ~45 min/day triaging. Hot leads still sit 6 h on average before first touch. If we cut that to 1 h, our models suggest +9 % revenue.â€

â¸»

Maria (Growth) â€“ Market Angle

â€œCompetitors price lead-scoring add-ons at $50â€“$150/seat. Position ours as included in Pro tier to drive upgrades from Basic; upsell Insight Packs later.â€

â¸»

Usman (ML) â€“ Data Reality Check

â€œWe have 48 k labeled leads over 24 months. 2 k unique conversions â†’ class imbalance 1:23.
First-principles remedy:
	â€¢	Use focal loss & SMOTE-NC.
	â€¢	Engineer temporal features (avg pageviews last 7 days, recency decay).
	â€¢	SHAP for explainability; log every inference for online learning.â€

â¸»

Ali (Backend) â€“ Architectural Options

â€œTwo paths:
	1.	Monolith Extension â€“ quick, but limits model upgrades.
	2.	Event-Driven Microservice â€“ Kafka queue, model hosted on ECS, async call-back.
We pick #2; gives plug-and-play for future â€˜next-best-actionâ€™ models.â€

â¸»

Tania (Frontend) â€“ UX Principles

â€œBadges alone wonâ€™t cut it. Weâ€™ll surface why a lead is â€˜Hotâ€™ in one click. Minimalist: color-coded badge + info-icon â†’ modal with SHAP-derived bullet list.â€

â¸»

Hassan (QA) â€“ Quality Gates

â€œDefine acceptance criteria now:
	â€¢	P90 inference latency < 800 ms.
	â€¢	Badge matches DB value 100 %.
	â€¢	Tooltips render within 50 ms of hover.
	â€¢	Model expl fails gracefully with stub.â€

â¸»

Decision Round-up
	1.	Adopt event-driven microservice (Ali).
	2.	Ship Phase 1 by July 5 to staging, GA July 19.
	3.	Bundle in Pro tier; marketing launch blog + webinar mid-July.
	4.	Track SQL-to-Win lift weekly; fallback to rules-based if lift < 5 % after 30 days.
	5.	Log every inference to S3 for continuous retraining loop.

â¸»

Action Items

Owner	Task	ETA
Usman	Prototype model v1 with SHAP	13 Jun
Ali	Kafka topics & service scaffold	14 Jun
Tania	UI wireframes â†’ Figma	12 Jun
Hassan	Draft test matrix	12 Jun
Maria	Pricing page update draft	15 Jun


â¸»

My Take (opinion)

Solid first-principles start: clear why->what->how chain, crisp metric, and explicit rollback guardrail. Biggest risk is data drift once reps change behavior because of the scoresâ€”plan an online-learning cadence sooner rather than later.
---


ğŸ•˜ Daily Scrum - Thursday, 6 June 2025, 11:00 AM (PKT)

Participants:
	â€¢	Sarah (Product Manager)
	â€¢	Ali (Backend Engineer)
	â€¢	Tania (Frontend Engineer)
	â€¢	Usman (ML Engineer)
	â€¢	Hassan (QA Engineer)

â¸»

Sarah (PM):
â€œAlright team, quick round today. Letâ€™s sync on the AI Lead Scoring feature for our CRM. Ali, can you go first?â€

â¸»

Ali (Backend):
â€œSure. Yesterday, I completed the REST endpoint for /leads/score. It accepts the POST payload with lead ID, context object, and optional override flags. The service now asynchronously queues the request into Kafka â€” topic lead-score-requests â€” and expects the score from Usmanâ€™s scoring microservice via the lead-score-responses topic.â€

â€œThe async response is written back to our PostgreSQL lead_scores table, with columns: lead_id, score, confidence, explanation_json, and timestamp. Iâ€™m exposing a polling endpoint /leads/score/:id/status for the frontend to check processing state â€” itâ€™s currently stubbed but will return either PENDING, SUCCESS, or FAILED.â€

â€œToday, Iâ€™ll work on securing the endpoint with JWT scopes. Iâ€™ll use our existing middleware for crm.write.leads. ETA for prod-ready endpoint: 2 days.â€

â¸»

Sarah:
â€œGood. Letâ€™s make sure the explanation in explanation_json is human-readable. Tania?â€

â¸»

Tania (Frontend):
â€œIâ€™m working on the Lead Detail view in React. When the lead is scored, we show a badge: Hot, Warm, or Cold, based on the score threshold â€” above 80 is Hot, below 40 is Cold.â€

â€œWe also added a tooltip using react-tooltip that parses the explanation JSON. It includes rules like â€˜Visited pricing page 3+ timesâ€™, â€˜Opened email campaign Xâ€™, etc.â€

â€œIâ€™m building a horizontal bar chart with Recharts for visualizing scoring breakdown by feature importance. Usman, Iâ€™ll need the schema of explanation_json to map it properly.â€

â¸»

Usman (ML Engineer):
â€œSure, explanation JSON is structured as:

{
  'feature_importance': {
    'page_views': 0.45,
    'email_opens': 0.3,
    'demo_requested': 0.2,
    'form_filled': 0.05
  },
  'final_score': 88,
  'confidence': 0.92
}

â€œIâ€™m using XGBoost for now with SHAP values to generate the importance. Trained on 2 years of sales-labeled leads, using 23 features â€” behavioral and firmographic. Model is versioned via MLflow: model:v12. Itâ€™s hosted on FastAPI in a container on ECS.â€

â€œI deployed the endpoint /score-lead which accepts the full lead JSON. Response includes score, explanation, and confidence. Today, Iâ€™m adding an A/B toggle in the API for fallback model model:v11 just in case.â€

â¸»

Sarah:
â€œExcellent. Can we log the SHAP explanations in our feature store for retraining?â€

â¸»

Usman:
â€œYes. Iâ€™ll push each scoring instance to our lead_scoring_logs table in S3 via Glue job daily.â€

â¸»

Hassan (QA):
â€œIâ€™ve written Cypress tests for score badge rendering, tooltip validation, and polling behavior. Iâ€™ll test edge cases today â€” like malformed JSON in explanation, API timeouts, and fallback model switches.â€

â¸»

Sarah:
â€œPerfect. Letâ€™s align tomorrow on release readiness. We need this in staging by Friday EOD. Any blockers?â€

â¸»

Ali:
â€œNone from me.â€

â¸»

Tania:
â€œAll good, just need final CSS tweaks on the tooltip.â€

â¸»

Usman:
â€œIâ€™ll document the API schema on Confluence today.â€

â¸»

Hassan:
â€œIâ€™ll review that once itâ€™s up.â€

â¸»

Sarah:
â€œGreat. Thanks team. Letâ€™s sync tomorrow â€” same time.â€
